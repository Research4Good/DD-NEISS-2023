{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcad5b9",
   "metadata": {
    "papermill": {
     "duration": 0.0028,
     "end_time": "2023-10-04T17:47:42.822542",
     "exception": false,
     "start_time": "2023-10-04T17:47:42.819742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3e98a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T17:47:42.830303Z",
     "iopub.status.busy": "2023-10-04T17:47:42.829956Z",
     "iopub.status.idle": "2023-10-04T17:47:45.501804Z",
     "shell.execute_reply": "2023-10-04T17:47:45.500837Z"
    },
    "papermill": {
     "duration": 2.679124,
     "end_time": "2023-10-04T17:47:45.503901",
     "exception": false,
     "start_time": "2023-10-04T17:47:42.824777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMB=[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcb7f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T17:47:45.511737Z",
     "iopub.status.busy": "2023-10-04T17:47:45.511336Z",
     "iopub.status.idle": "2023-10-05T05:47:47.802450Z"
    },
    "papermill": {
     "duration": 43200.005375,
     "end_time": "2023-10-05T05:47:45.512136",
     "exception": false,
     "start_time": "2023-10-04T17:47:45.506761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random, torch, tf, os packages seeded\n",
      "\n",
      "\n",
      "- torch, tf, os, sys, subprocess loaded \n",
      "- np, pol, pd, time, loaded\n",
      "\n",
      "\n",
      "ngpus = install_packages() \n",
      "\n",
      "seed_everything(1119) \n",
      "Running:\n",
      "\t pip install wget\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=551a7ae4140f6977f975e084960453bf0964078341c7ecf56402658da04edb63\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "reading decoded_df2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/1767350366.py:23: DtypeWarning: Columns (9,12,13,15,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  decoded_df2=pd.read_csv('/kaggle/input/neiss-sentence-transform-embeddings/decoded_df2__l1.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=======================0=======================\n",
      "\n",
      "\n",
      "(21335,) (size of feat extract)\n",
      "128 dimensions\n",
      "src 6 done in  324.96849706967674 written to narrative_cleaned_n426691_emb6_d0_2023-10-04\n",
      "\n",
      "\n",
      "=======================1=======================\n",
      "\n",
      "\n",
      "(21335,) (size of feat extract)\n",
      "128 dimensions\n",
      "src 6 done in  350.60339578787483 written to narrative_cleaned_n426691_emb6_d1_2023-10-04\n",
      "\n",
      "\n",
      "=======================2=======================\n",
      "\n",
      "\n",
      "(21335,) (size of feat extract)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 159\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[1;32m    158\u001b[0m multi_pool \u001b[38;5;241m=\u001b[39m Pool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m                 \n\u001b[1;32m    160\u001b[0m multi_pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    161\u001b[0m multi_pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm \n",
    "from time import time\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "import pickle, json\n",
    "\n",
    "if ('decoded_df2' in globals())==False:\n",
    "    print('reading decoded_df2...')\n",
    "       \n",
    "    decoded_df2=pd.read_csv('decoded_df2_unique.csv')\n",
    "    \n",
    "    cpcs_nums = decoded_df2.cpsc_case_number\n",
    "    size_n = decoded_df2.shape[0]\n",
    "    \n",
    "    Embeddings={}\n",
    "\n",
    "def embed( model, sentences, TF_MODEL ):             \n",
    "    if TF_MODEL==1:\n",
    "        #embeddings = model.predict( sentences )\n",
    "        embeddings = model(sentences)\n",
    "    else: # pytorch         \n",
    "        embeddings = model.encode(sentences)            \n",
    "    return embeddings\n",
    "\n",
    "def normalization(embeds):\n",
    "    norms = np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "    return embeds/norms\n",
    "\n",
    "# ======== https://github.com/huggingface/transformers/issues/15038 \n",
    "class NoDaemonProcess( mp.Process):\n",
    "    @property\n",
    "    def daemon(self):\n",
    "        return False\n",
    "    @daemon.setter\n",
    "    def daemon(self, value):\n",
    "        pass\n",
    "\n",
    "class NoDaemonContext(type( mp.get_context(\"fork\"))):\n",
    "    Process = NoDaemonProcess\n",
    "\n",
    "    \n",
    "D = 20    \n",
    "for src in [ 'narrative_cleaned']: #,'narrative' ]:            \n",
    "    for ibat in range(10):\n",
    "        print( f'\\n\\n======================={ibat}=======================\\n\\n')\n",
    "        for emb in EMB:                    \n",
    "            if INTERACTIVE:        \n",
    "                inp = decoded_df2[src][:30] \n",
    "            else:\n",
    "                inp =  decoded_df2[src][ibat::D]  \n",
    "            if 'cleaned' not in src:        \n",
    "                inp = inp.str.lower()\n",
    "            print( inp.shape, '(size of feat extract)')\n",
    "            inp = list(inp)            \n",
    "            starttime=time()           \n",
    "            TF_MODEL = 0\n",
    "            \n",
    "            if emb<4: \n",
    "                try:\n",
    "                    from sentence_transformers import SentenceTransformer\n",
    "                except:                \n",
    "                    install_packages( ['pip install -U sentence-transformers'], INTERACTIVE ); \n",
    "                    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "                if emb==1:# 768\n",
    "                    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "                elif emb==2: # 384\n",
    "                    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "                elif emb==3: #768\n",
    "                    model = SentenceTransformer('paraphrase-mpnet-base-v2')  # already explored paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "            elif emb==4: # 512\n",
    "                TF_MODEL = 1\n",
    "                import tensorflow_hub as hub\n",
    "                # Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Céspedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. \n",
    "                # Universal Sentence Encoder. arXiv:1803.11175, 2018.\n",
    "                module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"         \n",
    "                model = hub.load(module_url)       \n",
    "\n",
    "            elif emb==5:\n",
    "                # [1] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Narveen Ari, Wei Wang. Language-agnostic BERT Sentence Embedding. July 2020\n",
    "                import tensorflow_hub as hub            \n",
    "                try:\n",
    "                    import tensorflow_text as text  # Registers the ops.\n",
    "                except:\n",
    "                    install_packages(['pip install tensorflow_text'], INTERACTIVE)\n",
    "                    import tensorflow_text as text  # Registers the ops.\n",
    "\n",
    "                from transformers import pipeline \n",
    "                tokenizer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2')\n",
    "                model = hub.KerasLayer('https://tfhub.dev/google/LaBSE/2')\n",
    "                \n",
    "                def compute( inp ):                                       \n",
    "                    #print( end=f'{inp} ?', flush=True )\n",
    "                    res= model( tokenizer( inp ) )                    \n",
    "                    r=normalization( res['default']  )\n",
    "                    return np.array(r)                                               \n",
    "                mp_context = NoDaemonContext()                \n",
    "                nprocs = mp.cpu_count()\n",
    "                print( nprocs, 'cores' )\n",
    "                with mp_context.Pool( nprocs ) as pool:\n",
    "                    async_r = [pool.apply_async( compute, ii ).get() for i in tqdm(range(size_n)) for ii in inp[i] ] \n",
    "                    print( end='.' )\n",
    "                \n",
    "                '''\n",
    "                def compute( inp ):\n",
    "                    pip = get_pip() \n",
    "                pool = Pool( proprocess = 3 )\n",
    "                preds = pool.map( compute, inp )\n",
    "                pool.close()\n",
    "                pool.join()                \n",
    "                Embeddings[emb]= preds\n",
    "                '''                \n",
    "            elif emb>=6:\n",
    "\n",
    "                import torch                                            \n",
    "                from transformers import pipeline\n",
    "                from torch.multiprocessing import Pool, Process, set_start_method\n",
    "                #set_start_method(\"spawn\", force=True)\n",
    "\n",
    "                from transformers import BertModel, BertTokenizerFast         \n",
    "                names ={}\n",
    "                names[6] = \"setu4993/LEALLA-small\"\n",
    "                names[7] = \"setu4993/LEALLA-base\"\n",
    "                names[8] = \"setu4993/LEALLA-large\"                    \n",
    "\n",
    "                def get_pipe():\n",
    "                    tokenizer = BertTokenizerFast.from_pretrained(names[emb])\n",
    "                    model =  BertModel.from_pretrained(names[emb])\n",
    "                    model = model.eval()  \n",
    "                    return tokenizer, model\n",
    "                \n",
    "                def compute( inp ):\n",
    "                    tokenizer, model = get_pipe()                    \n",
    "                    #print(len(inp), end=f' {inp}', )\n",
    "                    english_inputs = tokenizer(inp, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        english_outputs = model(**english_inputs)\n",
    "                    r = np.array( english_outputs.pooler_output )    \n",
    "                    return r\n",
    "                \n",
    "                multi_pool = Pool(processes=3)\n",
    "                predictions = multi_pool.map( compute, inp)                 \n",
    "                multi_pool.close()\n",
    "                multi_pool.join()\n",
    "                \n",
    "                #m = predictions[0].shape[1] \n",
    "                #p = np.zeros( (size_n, m) )\n",
    "                #predictions = [ p[i,:]=j for i,j in enumerate( predictions ) ]                \n",
    "                Embeddings[emb]=np.array(predictions).squeeze()\n",
    "                \n",
    "        if emb<5:            \n",
    "            Embeddings[emb] = embed( model, inp, TF_MODEL )            \n",
    "\n",
    "        exec_time = (time() - starttime)/60 \n",
    "\n",
    "        d=Embeddings[emb].shape[1]\n",
    "        print(d,'dimensions')\n",
    "\n",
    "        pref = f'{src}_n{size_n}_emb{emb}_d{ibat}_{today}'\n",
    "        f=open( pref+'.txt' ,'w')\n",
    "        f.write( 'Finished in %2.f minutes'%exec_time )\n",
    "        f.close()\n",
    "\n",
    "        with open( pref+'.pkl', 'wb') as handle:\n",
    "            pickle.dump( {\"embeddings\": Embeddings[emb] } , handle)   \n",
    "        print('src', emb, 'done in ', exec_time, 'written to', pref )                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45d12e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2863cd2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba65e0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d620c61",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb6bec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43208.594144,
   "end_time": "2023-10-05T05:47:48.317597",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-04T17:47:39.723453",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
