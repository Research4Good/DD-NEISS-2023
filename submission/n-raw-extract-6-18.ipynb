{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a390e4f",
   "metadata": {
    "papermill": {
     "duration": 0.002831,
     "end_time": "2023-10-05T07:30:34.316761",
     "exception": false,
     "start_time": "2023-10-05T07:30:34.313930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c9c4aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T07:30:34.325592Z",
     "iopub.status.busy": "2023-10-05T07:30:34.324899Z",
     "iopub.status.idle": "2023-10-05T07:30:36.842187Z",
     "shell.execute_reply": "2023-10-05T07:30:36.840574Z"
    },
    "papermill": {
     "duration": 2.525524,
     "end_time": "2023-10-05T07:30:36.844862",
     "exception": false,
     "start_time": "2023-10-05T07:30:34.319338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMB=[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3773dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T07:30:36.853610Z",
     "iopub.status.busy": "2023-10-05T07:30:36.853197Z",
     "iopub.status.idle": "2023-10-05T18:48:36.826346Z",
     "shell.execute_reply": "2023-10-05T18:48:36.824731Z"
    },
    "papermill": {
     "duration": 40679.983847,
     "end_time": "2023-10-05T18:48:36.831778",
     "exception": false,
     "start_time": "2023-10-05T07:30:36.847931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random, torch, tf, os packages seeded\n",
      "\n",
      "\n",
      "- torch, tf, os, sys, subprocess loaded \n",
      "- np, pol, pd, time, loaded\n",
      "\n",
      "\n",
      "ngpus = install_packages() \n",
      "\n",
      "seed_everything(1119) \n",
      "Running:\n",
      "\t pip install wget\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=a6506dd89ef86c14ad84b97601a66de45a474f2b657f720a6c0c6b45cc1c4ec6\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "reading decoded_df2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/3741577042.py:23: DtypeWarning: Columns (9,12,13,15,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  decoded_df2=pd.read_csv('/kaggle/input/neiss-sentence-transform-embeddings/decoded_df2__l1.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=======================18=======================\n",
      "\n",
      "\n",
      "(21334,) (size of feat extract)\n",
      "128 dimensions\n",
      "src 6 done in  336.7452418645223 written to narrative_cleaned_n426691_emb6_d18_2023-10-05\n",
      "\n",
      "\n",
      "=======================19=======================\n",
      "\n",
      "\n",
      "(21334,) (size of feat extract)\n",
      "128 dimensions\n",
      "src 6 done in  340.55385416348776 written to narrative_cleaned_n426691_emb6_d19_2023-10-05\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm \n",
    "from time import time\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "import pickle, json\n",
    "\n",
    "\n",
    "if ('decoded_df2' in globals())==False:\n",
    "    print('reading decoded_df2...')\n",
    "       \n",
    "    decoded_df2=pd.read_csv('decoded_df2_unique.csv')\n",
    "    \n",
    "    cpcs_nums = decoded_df2.cpsc_case_number\n",
    "    size_n = decoded_df2.shape[0]\n",
    "    \n",
    "    Embeddings={}\n",
    "\n",
    "def embed( model, sentences, TF_MODEL ):             \n",
    "    if TF_MODEL==1:\n",
    "        #embeddings = model.predict( sentences )\n",
    "        embeddings = model(sentences)\n",
    "    else: # pytorch         \n",
    "        embeddings = model.encode(sentences)            \n",
    "    return embeddings\n",
    "\n",
    "def normalization(embeds):\n",
    "    norms = np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "    return embeds/norms\n",
    "\n",
    "# ======== https://github.com/huggingface/transformers/issues/15038 \n",
    "class NoDaemonProcess( mp.Process):\n",
    "    @property\n",
    "    def daemon(self):\n",
    "        return False\n",
    "    @daemon.setter\n",
    "    def daemon(self, value):\n",
    "        pass\n",
    "\n",
    "class NoDaemonContext(type( mp.get_context(\"fork\"))):\n",
    "    Process = NoDaemonProcess\n",
    "\n",
    "    \n",
    "D = 20    \n",
    "for src in [ 'narrative_cleaned']: #,'narrative' ]:            \n",
    "    for ibat in [18,19]:\n",
    "        print( f'\\n\\n======================={ibat}=======================\\n\\n')\n",
    "        for emb in EMB:                    \n",
    "            if INTERACTIVE:        \n",
    "                inp = decoded_df2[src][:30] \n",
    "            else:\n",
    "                inp =  decoded_df2[src][ibat::D]  \n",
    "            if 'cleaned' not in src:        \n",
    "                inp = inp.str.lower()\n",
    "            print( inp.shape, '(size of feat extract)')\n",
    "            inp = list(inp)            \n",
    "            starttime=time()           \n",
    "            TF_MODEL = 0\n",
    "            \n",
    "            if emb<4: \n",
    "                try:\n",
    "                    from sentence_transformers import SentenceTransformer\n",
    "                except:                \n",
    "                    install_packages( ['pip install -U sentence-transformers'], INTERACTIVE ); \n",
    "                    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "                if emb==1:# 768\n",
    "                    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "                elif emb==2: # 384\n",
    "                    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "                elif emb==3: #768\n",
    "                    model = SentenceTransformer('paraphrase-mpnet-base-v2')  # already explored paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "            elif emb==4: # 512\n",
    "                TF_MODEL = 1\n",
    "                import tensorflow_hub as hub\n",
    "                # Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-CÃ©spedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. \n",
    "                # Universal Sentence Encoder. arXiv:1803.11175, 2018.\n",
    "                module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"         \n",
    "                model = hub.load(module_url)       \n",
    "\n",
    "            elif emb==5:\n",
    "                # [1] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Narveen Ari, Wei Wang. Language-agnostic BERT Sentence Embedding. July 2020\n",
    "                import tensorflow_hub as hub            \n",
    "                try:\n",
    "                    import tensorflow_text as text  # Registers the ops.\n",
    "                except:\n",
    "                    install_packages(['pip install tensorflow_text'], INTERACTIVE)\n",
    "                    import tensorflow_text as text  # Registers the ops.\n",
    "\n",
    "                from transformers import pipeline \n",
    "                tokenizer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2')\n",
    "                model = hub.KerasLayer('https://tfhub.dev/google/LaBSE/2')\n",
    "                \n",
    "                def compute( inp ):                                       \n",
    "                    #print( end=f'{inp} ?', flush=True )\n",
    "                    res= model( tokenizer( inp ) )                    \n",
    "                    r=normalization( res['default']  )\n",
    "                    return np.array(r)                                               \n",
    "                mp_context = NoDaemonContext()                \n",
    "                nprocs = mp.cpu_count()\n",
    "                print( nprocs, 'cores' )\n",
    "                with mp_context.Pool( nprocs ) as pool:\n",
    "                    async_r = [pool.apply_async( compute, ii ).get() for i in tqdm(range(size_n)) for ii in inp[i] ] \n",
    "                    print( end='.' )\n",
    "                \n",
    "                '''\n",
    "                def compute( inp ):\n",
    "                    pip = get_pip() \n",
    "                pool = Pool( proprocess = 3 )\n",
    "                preds = pool.map( compute, inp )\n",
    "                pool.close()\n",
    "                pool.join()                \n",
    "                Embeddings[emb]= preds\n",
    "                '''                \n",
    "            elif emb>=6:\n",
    "\n",
    "                import torch                                            \n",
    "                from transformers import pipeline\n",
    "                from torch.multiprocessing import Pool, Process, set_start_method\n",
    "                #set_start_method(\"spawn\", force=True)\n",
    "\n",
    "                from transformers import BertModel, BertTokenizerFast         \n",
    "                names ={}\n",
    "                names[6] = \"setu4993/LEALLA-small\"\n",
    "                names[7] = \"setu4993/LEALLA-base\"\n",
    "                names[8] = \"setu4993/LEALLA-large\"                    \n",
    "\n",
    "                def get_pipe():\n",
    "                    tokenizer = BertTokenizerFast.from_pretrained(names[emb])\n",
    "                    model =  BertModel.from_pretrained(names[emb])\n",
    "                    model = model.eval()  \n",
    "                    return tokenizer, model\n",
    "                \n",
    "                def compute( inp ):\n",
    "                    tokenizer, model = get_pipe()                    \n",
    "                    #print(len(inp), end=f' {inp}', )\n",
    "                    english_inputs = tokenizer(inp, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        english_outputs = model(**english_inputs)\n",
    "                    r = np.array( english_outputs.pooler_output )    \n",
    "                    return r\n",
    "                \n",
    "                multi_pool = Pool(processes=3)\n",
    "                predictions = multi_pool.map( compute, inp)                 \n",
    "                multi_pool.close()\n",
    "                multi_pool.join()\n",
    "                \n",
    "                #m = predictions[0].shape[1] \n",
    "                #p = np.zeros( (size_n, m) )\n",
    "                #predictions = [ p[i,:]=j for i,j in enumerate( predictions ) ]                \n",
    "                Embeddings[emb]=np.array(predictions).squeeze()\n",
    "                \n",
    "        if emb<5:            \n",
    "            Embeddings[emb] = embed( model, inp, TF_MODEL )            \n",
    "\n",
    "        exec_time = (time() - starttime)/60 \n",
    "\n",
    "        d=Embeddings[emb].shape[1]\n",
    "        print(d,'dimensions')\n",
    "\n",
    "        pref = f'{src}_n{size_n}_emb{emb}_d{ibat}_{today}'\n",
    "        f=open( pref+'.txt' ,'w')\n",
    "        f.write( 'Finished in %2.f minutes'%exec_time )\n",
    "        f.close()\n",
    "\n",
    "        with open( pref+'.pkl', 'wb') as handle:\n",
    "            pickle.dump( {\"embeddings\": Embeddings[emb] } , handle)   \n",
    "        print('src', emb, 'done in ', exec_time, 'written to', pref )                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ccd5b",
   "metadata": {
    "papermill": {
     "duration": 0.002966,
     "end_time": "2023-10-05T18:48:36.837995",
     "exception": false,
     "start_time": "2023-10-05T18:48:36.835029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe5ad2",
   "metadata": {
    "papermill": {
     "duration": 0.002904,
     "end_time": "2023-10-05T18:48:36.844044",
     "exception": false,
     "start_time": "2023-10-05T18:48:36.841140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1ddb5",
   "metadata": {
    "papermill": {
     "duration": 0.002872,
     "end_time": "2023-10-05T18:48:36.850051",
     "exception": false,
     "start_time": "2023-10-05T18:48:36.847179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae1d82",
   "metadata": {
    "papermill": {
     "duration": 0.002799,
     "end_time": "2023-10-05T18:48:36.855986",
     "exception": false,
     "start_time": "2023-10-05T18:48:36.853187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265238f7",
   "metadata": {
    "papermill": {
     "duration": 0.003159,
     "end_time": "2023-10-05T18:48:36.862150",
     "exception": false,
     "start_time": "2023-10-05T18:48:36.858991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40688.828021,
   "end_time": "2023-10-05T18:48:39.805261",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-05T07:30:30.977240",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
