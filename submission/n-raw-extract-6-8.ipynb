{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7461e",
   "metadata": {
    "papermill": {
     "duration": 0.00357,
     "end_time": "2023-10-05T07:22:02.578710",
     "exception": false,
     "start_time": "2023-10-05T07:22:02.575140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4565c579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T07:22:02.589137Z",
     "iopub.status.busy": "2023-10-05T07:22:02.588600Z",
     "iopub.status.idle": "2023-10-05T07:22:05.249741Z",
     "shell.execute_reply": "2023-10-05T07:22:05.248649Z"
    },
    "papermill": {
     "duration": 2.669743,
     "end_time": "2023-10-05T07:22:05.252106",
     "exception": false,
     "start_time": "2023-10-05T07:22:02.582363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMB=[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad108439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T07:22:05.259750Z",
     "iopub.status.busy": "2023-10-05T07:22:05.259335Z",
     "iopub.status.idle": "2023-10-05T19:22:07.335506Z"
    },
    "papermill": {
     "duration": 43200.005292,
     "end_time": "2023-10-05T19:22:05.260199",
     "exception": false,
     "start_time": "2023-10-05T07:22:05.254907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random, torch, tf, os packages seeded\n",
      "\n",
      "\n",
      "- torch, tf, os, sys, subprocess loaded \n",
      "- np, pol, pd, time, loaded\n",
      "\n",
      "\n",
      "ngpus = install_packages() \n",
      "\n",
      "seed_everything(1119) \n",
      "Running:\n",
      "\t pip install wget\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=7c7b9b3f287a845565b57779985cb508c5f6ce7e16c7d6e255634f1cc56659bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "reading decoded_df2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/516521326.py:23: DtypeWarning: Columns (9,12,13,15,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  decoded_df2=pd.read_csv('/kaggle/input/neiss-sentence-transform-embeddings/decoded_df2__l1.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=======================8=======================\n",
      "\n",
      "\n",
      "(21335,) (size of feat extract)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f95b00c9-c6e6-4fa4-9aba-9a8577f57431)')' thrown while requesting HEAD https://huggingface.co/setu4993/LEALLA-small/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 dimensions\n",
      "src 6 done in  362.1035445412 written to narrative_cleaned_n426691_emb6_d8_2023-10-05\n",
      "\n",
      "\n",
      "=======================9=======================\n",
      "\n",
      "\n",
      "(21335,) (size of feat extract)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/tmp/ipykernel_20/516521326.py\", line 149, in compute\n",
      "    tokenizer, model = get_pipe()\n",
      "  File \"/tmp/ipykernel_20/516521326.py\", line 144, in get_pipe\n",
      "    model =  BertModel.from_pretrained(names[emb])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2883, in from_pretrained\n",
      "    state_dict = load_state_dict(resolved_archive_file)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 478, in load_state_dict\n",
      "    return safe_load_file(checkpoint_file)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/safetensors/torch.py\", line 310, in load_file\n",
      "    result[k] = f.get_tensor(k)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/storage.py\", line 231, in __getitem__\n",
      "    def __getitem__(self, *args, **kwargs):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/tmp/ipykernel_20/516521326.py\", line 149, in compute\n",
      "    tokenizer, model = get_pipe()\n",
      "  File \"/tmp/ipykernel_20/516521326.py\", line 143, in get_pipe\n",
      "    tokenizer = BertTokenizerFast.from_pretrained(names[emb])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1777, in from_pretrained\n",
      "    resolved_config_file = cached_file(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 429, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1195, in hf_hub_download\n",
      "    metadata = get_hf_file_metadata(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1532, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 407, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 442, in _request_wrapper\n",
      "    return http_backoff(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 159\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[1;32m    158\u001b[0m multi_pool \u001b[38;5;241m=\u001b[39m Pool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m                 \n\u001b[1;32m    160\u001b[0m multi_pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    161\u001b[0m multi_pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm \n",
    "from time import time\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "import pickle, json\n",
    "\n",
    "\n",
    "if ('decoded_df2' in globals())==False:\n",
    "    print('reading decoded_df2...')\n",
    "       \n",
    "    decoded_df2=pd.read_csv('decoded_df2_unique.csv')\n",
    "    \n",
    "    cpcs_nums = decoded_df2.cpsc_case_number\n",
    "    size_n = decoded_df2.shape[0]\n",
    "    \n",
    "    Embeddings={}\n",
    "\n",
    "\n",
    "def embed( model, sentences, TF_MODEL ):             \n",
    "    if TF_MODEL==1:\n",
    "        #embeddings = model.predict( sentences )\n",
    "        embeddings = model(sentences)\n",
    "    else: # pytorch         \n",
    "        embeddings = model.encode(sentences)            \n",
    "    return embeddings\n",
    "\n",
    "def normalization(embeds):\n",
    "    norms = np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "    return embeds/norms\n",
    "\n",
    "# ======== https://github.com/huggingface/transformers/issues/15038 \n",
    "class NoDaemonProcess( mp.Process):\n",
    "    @property\n",
    "    def daemon(self):\n",
    "        return False\n",
    "    @daemon.setter\n",
    "    def daemon(self, value):\n",
    "        pass\n",
    "\n",
    "class NoDaemonContext(type( mp.get_context(\"fork\"))):\n",
    "    Process = NoDaemonProcess\n",
    "\n",
    "    \n",
    "D = 20    \n",
    "for src in [ 'narrative_cleaned']: #,'narrative' ]:            \n",
    "    for ibat in [8,9]:\n",
    "        print( f'\\n\\n======================={ibat}=======================\\n\\n')\n",
    "        for emb in EMB:                    \n",
    "            if INTERACTIVE:        \n",
    "                inp = decoded_df2[src][:30] \n",
    "            else:\n",
    "                inp =  decoded_df2[src][ibat::D]  \n",
    "            if 'cleaned' not in src:        \n",
    "                inp = inp.str.lower()\n",
    "            print( inp.shape, '(size of feat extract)')\n",
    "            inp = list(inp)            \n",
    "            starttime=time()           \n",
    "            TF_MODEL = 0\n",
    "            \n",
    "            if emb<4: \n",
    "                try:\n",
    "                    from sentence_transformers import SentenceTransformer\n",
    "                except:                \n",
    "                    install_packages( ['pip install -U sentence-transformers'], INTERACTIVE ); \n",
    "                    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "                if emb==1:# 768\n",
    "                    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "                elif emb==2: # 384\n",
    "                    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "                elif emb==3: #768\n",
    "                    model = SentenceTransformer('paraphrase-mpnet-base-v2')  # already explored paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "            elif emb==4: # 512\n",
    "                TF_MODEL = 1\n",
    "                import tensorflow_hub as hub\n",
    "                # Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-CÃ©spedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. \n",
    "                # Universal Sentence Encoder. arXiv:1803.11175, 2018.\n",
    "                module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"         \n",
    "                model = hub.load(module_url)       \n",
    "\n",
    "            elif emb==5:\n",
    "                # [1] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Narveen Ari, Wei Wang. Language-agnostic BERT Sentence Embedding. July 2020\n",
    "                import tensorflow_hub as hub            \n",
    "                try:\n",
    "                    import tensorflow_text as text  # Registers the ops.\n",
    "                except:\n",
    "                    install_packages(['pip install tensorflow_text'], INTERACTIVE)\n",
    "                    import tensorflow_text as text  # Registers the ops.\n",
    "\n",
    "                from transformers import pipeline \n",
    "                tokenizer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2')\n",
    "                model = hub.KerasLayer('https://tfhub.dev/google/LaBSE/2')\n",
    "                \n",
    "                def compute( inp ):                                       \n",
    "                    #print( end=f'{inp} ?', flush=True )\n",
    "                    res= model( tokenizer( inp ) )                    \n",
    "                    r=normalization( res['default']  )\n",
    "                    return np.array(r)                                               \n",
    "                mp_context = NoDaemonContext()                \n",
    "                nprocs = mp.cpu_count()\n",
    "                print( nprocs, 'cores' )\n",
    "                with mp_context.Pool( nprocs ) as pool:\n",
    "                    async_r = [pool.apply_async( compute, ii ).get() for i in tqdm(range(size_n)) for ii in inp[i] ] \n",
    "                    print( end='.' )\n",
    "                \n",
    "                '''\n",
    "                def compute( inp ):\n",
    "                    pip = get_pip() \n",
    "                pool = Pool( proprocess = 3 )\n",
    "                preds = pool.map( compute, inp )\n",
    "                pool.close()\n",
    "                pool.join()                \n",
    "                Embeddings[emb]= preds\n",
    "                '''                \n",
    "            elif emb>=6:\n",
    "\n",
    "                import torch                                            \n",
    "                from transformers import pipeline\n",
    "                from torch.multiprocessing import Pool, Process, set_start_method\n",
    "                #set_start_method(\"spawn\", force=True)\n",
    "\n",
    "                from transformers import BertModel, BertTokenizerFast         \n",
    "                names ={}\n",
    "                names[6] = \"setu4993/LEALLA-small\"\n",
    "                names[7] = \"setu4993/LEALLA-base\"\n",
    "                names[8] = \"setu4993/LEALLA-large\"                    \n",
    "\n",
    "                def get_pipe():\n",
    "                    tokenizer = BertTokenizerFast.from_pretrained(names[emb])\n",
    "                    model =  BertModel.from_pretrained(names[emb])\n",
    "                    model = model.eval()  \n",
    "                    return tokenizer, model\n",
    "                \n",
    "                def compute( inp ):\n",
    "                    tokenizer, model = get_pipe()                    \n",
    "                    #print(len(inp), end=f' {inp}', )\n",
    "                    english_inputs = tokenizer(inp, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        english_outputs = model(**english_inputs)\n",
    "                    r = np.array( english_outputs.pooler_output )    \n",
    "                    return r\n",
    "                \n",
    "                multi_pool = Pool(processes=3)\n",
    "                predictions = multi_pool.map( compute, inp)                 \n",
    "                multi_pool.close()\n",
    "                multi_pool.join()\n",
    "                \n",
    "                #m = predictions[0].shape[1] \n",
    "                #p = np.zeros( (size_n, m) )\n",
    "                #predictions = [ p[i,:]=j for i,j in enumerate( predictions ) ]                \n",
    "                Embeddings[emb]=np.array(predictions).squeeze()\n",
    "                \n",
    "        if emb<5:            \n",
    "            Embeddings[emb] = embed( model, inp, TF_MODEL )            \n",
    "\n",
    "        exec_time = (time() - starttime)/60 \n",
    "\n",
    "        d=Embeddings[emb].shape[1]\n",
    "        print(d,'dimensions')\n",
    "\n",
    "        pref = f'{src}_n{size_n}_emb{emb}_d{ibat}_{today}'\n",
    "        f=open( pref+'.txt' ,'w')\n",
    "        f.write( 'Finished in %2.f minutes'%exec_time )\n",
    "        f.close()\n",
    "\n",
    "        with open( pref+'.pkl', 'wb') as handle:\n",
    "            pickle.dump( {\"embeddings\": Embeddings[emb] } , handle)   \n",
    "        print('src', emb, 'done in ', exec_time, 'written to', pref )                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6a9af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1983a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2e8df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7aca3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3bf8a9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43208.796396,
   "end_time": "2023-10-05T19:22:08.037296",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-05T07:21:59.240900",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
